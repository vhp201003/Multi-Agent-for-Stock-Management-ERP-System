{
  "agents": {
    "orchestrator": {
      "model": "openai/gpt-oss-20b",
      "temperature": 0.5,
      "top_p": 0.9,
      "max_tokens": 2048,
      "frequency_penalty": 0.0,
      "presence_penalty": 0.0
    },
    "inventory": {
      "model": "openai/gpt-oss-20b",
      "mcp_server_url": "http://localhost:8011/mcp",
      "temperature": 0.5,
      "top_p": 0.9,
      "max_tokens": 2048,
      "frequency_penalty": 0.0,
      "presence_penalty": 0.0
    },
    "analytics": {
      "model": "openai/gpt-oss-20b",
      "mcp_server_url": "http://localhost:8012/mcp",
      "temperature": 0.6,
      "top_p": 0.9,
      "max_tokens": 2048,
      "frequency_penalty": 0.0,
      "presence_penalty": 0.0
    },
    "forecasting": {
      "model": "openai/gpt-oss-20b",
      "mcp_server_url": "http://localhost:8013/mcp",
      "temperature": 0.4,
      "top_p": 0.85,
      "max_tokens": 1024,
      "frequency_penalty": 0.0,
      "presence_penalty": 0.0,
      "reasoning_format": "hidden"
    },
    "ordering": {
      "model": "openai/gpt-oss-20b",
      "mcp_server_url": "http://localhost:8014/mcp",
      "temperature": 0.5,
      "top_p": 0.9,
      "max_tokens": 2048,
      "frequency_penalty": 0.0,
      "presence_penalty": 0.0,
      "reasoning_format": "hidden"
    },
    "chat_agent": {
      "model": "openai/gpt-oss-20b",
      "temperature": 0.7,
      "top_p": 0.95,
      "max_tokens": 4096,
      "frequency_penalty": 0.0,
      "presence_penalty": 0.0,
      "reasoning_format": "hidden"
    },
    "summary_agent": {
      "model": "openai/gpt-oss-20b",
      "temperature": 0.6,
      "top_p": 0.9,
      "max_tokens": 1024,
      "frequency_penalty": 0.0,
      "presence_penalty": 0.0,
      "reasoning_format": "hidden"
    }
  }
}
